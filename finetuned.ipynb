{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tune XLM-RoBERTa for Indic Sentiment Analysis\n",
        "\n",
        "This notebook fine-tunes `xlm-roberta-base` for sentiment analysis (positive, negative, neutral) on Indic languages (Hindi, Tamil, Telugu, Malayalam, Kannada) using the AI4Bharat IndicSentiment dataset. The model will be integrated with a Flask API for audio transcription and sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4\n",
        "!pip install pandas==2.2.2\n",
        "!pip install scikit-learn==1.5.2\n",
        "!pip install transformers==4.44.2 datasets==2.21.0 torch==2.4.1\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "logger.info(\"Environment setup completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Load and Preprocess Dataset\n",
        "\n",
        "Load the AI4Bharat IndicSentiment dataset, filter for Hindi, Tamil, Telugu, Malayalam, and Kannada, and preprocess the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "try:\n",
        "    dataset = load_dataset('ai4bharat/IndicSentiment', split='train')\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load dataset: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "target_languages = ['hi', 'ta', 'te', 'ml', 'kn']\n",
        "df = dataset.to_pandas()\n",
        "df = df[df['language'].isin(target_languages)]\n",
        "\n",
        "def clean_text(text):\n",
        "    import re\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = re.sub(r'http\\S+', '', text)  \n",
        "    text = re.sub(r'[^\\w\\s]', '', text) \n",
        "    text = re.sub(r'\\s+', ' ', text)  \n",
        "    return text.strip()\n",
        "\n",
        "# Map labels to integers (positive=2, neutral=1, negative=0)\n",
        "label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "df['label'] = df['sentiment'].map(label_map)\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Drop rows with empty text or invalid labels\n",
        "df = df.dropna(subset=['text', 'label'])\n",
        "df = df[df['text'] != '']\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df[['text', 'label', 'language']])\n",
        "\n",
        "# Split dataset\n",
        "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "test_val = train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
        "datasets = DatasetDict({\n",
        "    'train': train_test['train'],\n",
        "    'validation': test_val['train'],\n",
        "    'test': test_val['test']\n",
        "})\n",
        "\n",
        "logger.info(f\"Dataset splits: Train={len(datasets['train'])}, Validation={len(datasets['validation'])}, Test={len(datasets['test'])}\")\n",
        "logger.info(f\"Languages: {df['language'].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Tokenize Dataset\n",
        "\n",
        "Tokenize the text using the `xlm-roberta-base` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load tokenizer: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['text', 'language'])\n",
        "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
        "tokenized_datasets.set_format('torch')\n",
        "\n",
        "logger.info(\"Dataset tokenized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Initialize Model\n",
        "\n",
        "Load `xlm-roberta-base` and configure it for 3-class classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        'xlm-roberta-base',\n",
        "        num_labels=3,\n",
        "        id2label={0: 'negative', 1: 'neutral', 2: 'positive'},\n",
        "        label2id={'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "    )\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load model: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "logger.info(\"Model initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Define Metrics\n",
        "\n",
        "Define functions to compute accuracy and F1-score for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "logger.info(\"Metrics defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Configure Training\n",
        "\n",
        "Set up training arguments for the `Trainer` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/indic_sentiment_model',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1'\n",
        ")\n",
        "\n",
        "logger.info(\"Training arguments configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Train Model\n",
        "\n",
        "Initialize the `Trainer` and start fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "logger.info(\"Training completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Evaluate Model\n",
        "\n",
        "Evaluate the model on the test set and display the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    eval_results = trainer.evaluate(tokenized_datasets['test'])\n",
        "    logger.info(f\"Test results: {eval_results}\")\n",
        "\n",
        "    predictions = trainer.predict(tokenized_datasets['test'])\n",
        "    preds = np.argmax(predictions.predictions, axis=-1)\n",
        "    cm = confusion_matrix(predictions.label_ids, preds)\n",
        "    logger.info(f\"Confusion Matrix:\\n{cm}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Evaluation failed: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Save Model\n",
        "\n",
        "Save the fine-tuned model and tokenizer to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    model.save_pretrained('/content/drive/MyDrive/indic_sentiment_model/final')\n",
        "    tokenizer.save_pretrained('/content/drive/MyDrive/indic_sentiment_model/final')\n",
        "    logger.info(\"Model and tokenizer saved to Google Drive\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to save model: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Test Model\n",
        "\n",
        "Test the fine-tuned model with long, phone call-like sentences for each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "try:\n",
        "    sentiment_pipeline = pipeline(\n",
        "        'sentiment-analysis',\n",
        "        model='/content/drive/MyDrive/indic_sentiment_model/final',\n",
        "        tokenizer='/content/drive/MyDrive/indic_sentiment_model/final'\n",
        "    )\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load pipeline: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "test_sentences = [\n",
        "    {\n",
        "        'text': 'मैंने आपके ग्राहक सेवा दल के साथ अभी-अभी एक शानदार अनुभव प्राप्त किया; वे इतने मददगार और धैर्यवान थे, और मेरी समस्या को तुरंत हल कर दिया, मैं बहुत खुश हूँ!',\n",
        "        'language': 'hi',\n",
        "        'expected': 'positive'\n",
        "    },\n",
        "    {\n",
        "        'text': 'நான் உங்கள் வாடிக்கையாளர் சேவைக் குழுவுடன் இப்போது ஒரு அற்புதமான அனுபவத்தைப் பெற்றேன்; அவர்கள் மிகவும் உதவிகரமாகவும் பொறுமையாகவும் இருந்தனர், எனது பிரச்சினையை உடனடியாக தீர்த்து விட்டனர், நான் மிகவும் மகிழ்ச்சியாக இருக்கிறேன்!',\n",
        "        'language': 'ta',\n",
        "        'expected': 'positive'\n",
        "    },\n",
        "    {\n",
        "        'text': 'నేను మీ కస్టమర్ సర్వీస్ టీమ్‌తో ఇప్పుడే అద్భుతమైన అనుభవం పొందాను; వారు చాలా సహాయకారిగా మరియు ఓపికగా ఉన్నారు, నా సమస్యను వెంటనే పరిష్కరించారు, నేను చాలా సంతోషంగా ఉన్నాను!',\n",
        "        'language': 'te',\n",
        "        'expected': 'positive'\n",
        "    },\n",
        "    {\n",
        "        'text': 'ഞാൻ ഇപ്പോൾ നിന്റെ ഉപഭോക്തൃ സേവന ടീമിനോട് ഒരു മനോഹരമായ അനുഭവം നേടി; അവർ വളരെ സഹായകരവും ക്ഷമയോടെയും ആയിരുന്നു, എന്റെ പ്രശ്നം തൽക്ഷണം പരിഹരിച്ചു, ഞാൻ വളരെ സന്തോഷവാനാണ്!',\n",
        "        'language': 'ml',\n",
        "        'expected': 'positive'\n",
        "    },\n",
        "    {\n",
        "        'text': 'ನಾನು ಈಗ ತಾನೇ ನಿಮ್ಮ ಗ್ರಾಹಕ ಸೇವಾ ತಂಡದೊಂದಿಗೆ ಅದ್ಭುತವಾದ ಅನುಭವವನ್ನು ಪಡೆದೆ; ಅವರು ತುಂಬಾ ಸಹಾಯಕವಾಗಿದ್ದರು ಮತ್ತು ತಾಳ್ಮೆಯಿಂದ ಕೂಡಿದ್ದರು, ನನ್ನ ಸಮಸ್ಯೆಯನ್ನು ತಕ್ಷಣವೇ ಬಗೆಹರಿಸಿದರು, ನಾನು ತುಂಬಾ ಸಂತೋಷವಾಗಿದ್ದೇನೆ!',\n",
        "        'language': 'kn',\n",
        "        'expected': 'positive'\n",
        "    }\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    try:\n",
        "        result = sentiment_pipeline(sentence['text'])[0]\n",
        "        print(f\"Language: {sentence['language']}\")\n",
        "        print(f\"Text: {sentence['text']}\")\n",
        "        print(f\"Sentiment: {result['label']} (score: {result['score']:.4f})\")\n",
        "        print(f\"Expected: {sentence['expected']}\")\n",
        "        print('-' * 80)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to analyze '{sentence['text']}': {str(e)}\")\n",
        "        print(f\"Error analyzing '{sentence['text']}': {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
